<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Braintrust Efficacy Brief | Accenture Federal Services</title>
    <style>
        /* AFS Brand Colors - Minimal Use */
        :root {
            --core-purple: #7500C0;
            --accent-purple: #A100FF;
            --light-purple: #e6dcff;
            --text-black: #1a1a1a;
            --text-gray: #4a4a4a;
            --bg-white: #ffffff;
            --bg-subtle: #f8f8f8;
            --border-light: #e0e0e0;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Graphik', -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif;
            background: var(--bg-white);
            color: var(--text-black);
            line-height: 1.8;
            font-size: 16px;
        }

        p {
            font-size: 1.2em;
            color: var(--text-gray);
            margin-bottom: 20px;
            line-height: 1.8;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 60px 40px;
        }

        /* Hero Section - Brand Colors */
        header {
            background: linear-gradient(135deg, var(--core-purple) 0%, var(--accent-purple) 100%);
            color: white;
            padding: 50px;
            border-radius: 8px;
            margin-bottom: 50px;
            box-shadow: 0 4px 12px rgba(117, 0, 192, 0.15);
        }

        .wordmark {
            display: flex;
            align-items: center;
            gap: 25px;
            margin-bottom: 20px;
        }

        .gt-symbol {
            font-size: 64px;
            font-weight: bold;
            opacity: 0.9;
        }

        h1 {
            font-family: 'Graphik', Arial, sans-serif;
            font-weight: 700;
            font-size: 42px;
            margin-bottom: 12px;
            line-height: 1.2;
        }

        .subtitle {
            font-family: 'GT Sectra Fine', Georgia, serif;
            font-size: 28px;
            opacity: 0.95;
            margin-bottom: 8px;
            font-weight: 400;
        }

        .meta {
            font-size: 18px;
            opacity: 0.85;
            font-weight: 400;
        }

        /* Body Content - Minimal Colors */
        h2 {
            font-family: 'Graphik', Arial, sans-serif;
            font-weight: 600;
            font-size: 2.5em;
            color: var(--text-black);
            margin: 50px 0 20px 0;
            padding-bottom: 12px;
            border-bottom: 2px solid var(--border-light);
        }

        h3 {
            font-family: 'GT Sectra Fine', Georgia, serif;
            font-size: 1.9em;
            color: var(--text-black);
            margin: 35px 0 18px 0;
            font-weight: 600;
        }


        strong {
            color: var(--text-black);
            font-weight: 600;
        }

        em {
            font-style: italic;
            color: var(--text-gray);
        }

        /* Callout Boxes - Strategic Brand Color Use */
        .callout {
            background: var(--bg-subtle);
            border-left: 4px solid var(--core-purple);
            padding: 25px 30px;
            margin: 30px 0;
            border-radius: 4px;
        }

        .callout.critical {
            background: #fffaf0;
            border-left-color: #ff6b35;
        }

        .callout.success {
            background: #f0fff4;
            border-left-color: #28a745;
        }

        .callout > strong {
            color: var(--core-purple);
            display: block;
            margin-bottom: 10px;
            font-size: 17px;
        }

        .callout.critical > strong {
            color: #cc5500;
        }

        .callout.success > strong {
            color: #1e7d32;
        }

        .callout p {
            margin-bottom: 0;
        }

        .callout p strong {
            display: inline;
            color: var(--text-black);
            font-weight: 600;
            font-size: inherit;
        }

        /* Section Divider */
        hr {
            border: none;
            border-top: 1px solid var(--border-light);
            margin: 40px 0;
        }

        /* Footer */
        footer {
            border-top: 2px solid var(--border-light);
            padding-top: 30px;
            margin-top: 60px;
            text-align: center;
            color: var(--text-gray);
            font-size: 14px;
        }

        footer .gt-symbol {
            font-size: 32px;
            color: var(--core-purple);
            margin-top: 15px;
        }

        /* Links */
        a {
            color: var(--core-purple);
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin: 20px 0;
            padding-left: 25px;
            line-height: 1.8;
        }

        /* Assessment Table */
        .assessment-table-wrapper {
            background: var(--bg-subtle);
            border-left: 4px solid var(--core-purple);
            padding: 25px 30px;
            margin: 30px 0;
            border-radius: 4px;
        }

        .assessment-table-wrapper > strong {
            color: var(--core-purple);
            display: block;
            margin-bottom: 15px;
            font-size: 1.4em;
        }

        .assessment-table {
            width: 100%;
            border-collapse: collapse;
            background: white;
            border-radius: 4px;
            overflow: hidden;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }

        .assessment-table th {
            background: linear-gradient(135deg, var(--core-purple) 0%, var(--accent-purple) 100%);
            color: white;
            padding: 12px 15px;
            text-align: center;
            vertical-align: middle;
            font-weight: bold;
            font-size: 1.4em;
            border-right: 1px solid rgba(255,255,255,0.2);
        }

        .assessment-table th:last-child {
            border-right: none;
        }

        .assessment-table td {
            padding: 15px;
            border-right: 1px solid var(--border-light);
            border-bottom: 1px solid var(--border-light);
            vertical-align: top;
            font-size: 1.2em;
            line-height: 1.6;
        }

        .assessment-table td:last-child {
            border-right: none;
        }

        .assessment-table tr:last-child td {
            border-bottom: none;
        }

        .assessment-table em {
            color: var(--text-gray);
            font-style: italic;
            font-size: 1.2em;
        }

        /* Recommendation Table */
        .recommendation-wrapper {
            background: var(--bg-subtle);
            border-left: 4px solid var(--core-purple);
            padding: 25px 30px;
            margin: 30px 0;
            border-radius: 4px;
        }

        .recommendation-wrapper > strong {
            color: var(--core-purple);
            display: block;
            margin-bottom: 15px;
            font-size: 1.4em;
        }

        .recommendation-table {
            width: 100%;
            border-collapse: collapse;
            background: white;
            border-radius: 4px;
            overflow: hidden;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }

        .recommendation-table td {
            padding: 15px;
            border: 1px solid var(--border-light);
            vertical-align: middle;
            font-size: 1.2em;
            line-height: 1.6;
        }

        .recommendation-table td:first-child {
            font-weight: bold;
            background: var(--bg-subtle);
            width: 30%;
        }

        .recommendation-table .rec-value {
            color: #cc0000;
            font-weight: bold;
            font-size: 1.3em;
        }

        .recommendation-table .rec-instructions {
            font-size: 1.4em;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 40px 20px;
            }
            h1 {
                font-size: 32px;
            }
            h2 {
                font-size: 24px;
            }
            h3 {
                font-size: 20px;
            }
            .wordmark {
                flex-direction: column;
                align-items: flex-start;
            }
            header {
                padding: 35px 25px;
            }
            .assessment-table {
                font-size: 0.9em;
            }
            .assessment-table th,
            .assessment-table td {
                padding: 10px 8px;
                font-size: 0.9em;
            }
        }

        @media print {
            body {
                background: white;
            }
            header {
                background: white;
                border: 2px solid var(--core-purple);
                color: var(--text-black);
            }
            .gt-symbol {
                color: var(--core-purple);
            }
            .callout {
                border: 1px solid var(--border-light);
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <div class="wordmark">
                <span class="gt-symbol">&gt;</span>
                <div>
                    <h1>Braintrust Efficacy Brief</h1>
                    <div class="subtitle">Tool Evaluation</div>
                    <div class="meta">GenAI COTS Team | Accenture Federal Services | January 2025</div>
                </div>
            </div>
        </header>

        <section>
            <h2>Executive Summary</h2>

            <p>Braintrust is a cloud-based AI observability and evaluation platform that provides systematic evaluation infrastructure, production monitoring, and collaborative tooling for teams developing LLM-powered applications. The platform offers a hybrid deployment model where <strong>sensitive application data (prompts, outputs, traces) remains in customer-controlled infrastructure</strong> while the <strong>control plane (UI, metadata, experiment orchestration) resides in Braintrust's cloud</strong>.</p>

            <p>The vendor reports impressive adoption metrics with enterprise customers including Notion, Stripe, Zapier, Airtable, and Instacart. Customer case studies claim ~30%+ accuracy improvements in AI applications within weeks and ~80x faster queries compared to traditional database approaches. These represent vendor-reported optimal conditions reflecting systematic evaluation workflows compared to ad-hoc manual testing.</p>

            <p><strong>The single most critical finding</strong>: Braintrust lacks FedRAMP authorization and has no publicly stated roadmap toward federal compliance. While application data stays local, <strong>operational metadata (project names, experiment identifiers, dataset names, organization information, hashed API keys) flows to Braintrust's cloud infrastructure</strong>. For federal environments handling CUI or classified information, this metadata transmission represents a fundamental blocker.</p>

            <div class="recommendation-wrapper">
                <strong>Recommendation Summary:</strong>
                <table class="recommendation-table">
                    <tr>
                        <td><strong>Recommendation:</strong></td>
                        <td><span class="rec-value">Wait and Watch</span> for federal; <span class="rec-value">Conditional Proceed</span> for commercial</td>
                    </tr>
                    <tr>
                        <td><strong>Explanation:</strong></td>
                        <td>Absence of FedRAMP authorization combined with metadata cloud dependency that likely constitutes CUI transmission for federal workloads</td>
                    </tr>
                </table>
            </div>

            <div class="assessment-table-wrapper">
                <strong>Decision Matrix:</strong>
                <table class="assessment-table">
                    <thead>
                        <tr>
                            <th>Alignment to Existing Stack</th>
                            <th>Differentiation</th>
                            <th>Security and Compliance</th>
                            <th>ROI Potential</th>
                            <th>Industry / Community Backing</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Fills observability/evaluation gap in AI development workflows; complements existing CI/CD and LLM infrastructure rather than replacing tools</td>
                            <td>Workflow automation and infrastructure optimization rather than proprietary AI innovation; uses third-party LLMs (OpenAI, Anthropic) behind unified interface</td>
                            <td><strong>SOC 2 Type II but no FedRAMP authorization</strong>; hybrid deployment keeps data local but <strong>metadata transmits to cloud</strong>; likely CUI transmission for federal use cases</td>
                            <td>Vendor reports ~30%+ accuracy improvements and ~80x faster queries; systematic evaluation vs. ad-hoc testing delivers measurable workflow improvements for organizations with multiple AI applications</td>
                            <td>$150M valuation with $45M funding (Andreessen Horowitz); enterprise customers include Notion, Stripe, Zapier, Airtable, Instacart; client base doubled in past year</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="callout critical">
                <strong>Critical Federal Blocker:</strong>
                <p>Braintrust currently has <strong>no FedRAMP authorization</strong> (not Ready, In Process, or Authorized). The hybrid deployment architecture transmits operational metadata to Braintrust's cloud infrastructure even when application data remains local. For federal workloads, this metadata likely qualifies as <strong>Controlled Unclassified Information (CUI)</strong> requiring protection. The metadata flow to non-FedRAMP infrastructure creates a compliance violation.</p>
                <p>The hybrid model requires internet connectivity between the customer's data plane and Braintrust's control plane for UI functionality. <strong>True air-gapped deployment is not supported under the documented architecture</strong> - Braintrust explicitly frames their "full deployment mode" (control plane + data plane self-hosted) as "only available for testing with the intent of using the hybrid configuration in production." This is a testing environment, not a production deployment option.</p>
            </div>

            <div class="callout">
                <strong>Assessment Context:</strong>
                <p>The local data plane does <strong>not</strong> eliminate FedRAMP requirements for federal use. Metadata is still data - project names, experiment identifiers, dataset names, and organizational information constitute CUI for many federal workloads. The control plane dependency means the UI and management functionality require connectivity to Braintrust's non-FedRAMP cloud infrastructure. Any federal information system component processing government data typically requires FedRAMP authorization when using cloud services.</p>
                <p>Organizations should not assume the local data plane deployment satisfies federal compliance requirements. Braintrust has no public evidence of FedRAMP authorization or active pursuit, and the company appears focused on commercial enterprise market rather than federal compliance.</p>
            </div>
        </section>

        <hr>

        <section>
            <h2>Bottom Line Recommendation</h2>

            <p><strong>Wait and Watch</strong> for federal environments; <strong>Conditional Proceed</strong> for commercial/unclassified use cases where cloud metadata transmission is permissible.</p>

            <p>Braintrust delivers genuine value for systematic AI evaluation and observability, particularly for organizations with mature DevOps practices and cloud-first infrastructure. The platform transforms ad-hoc prompt testing into reproducible, traceable evaluation workflows with measurable quality improvements. However, <strong>the lack of FedRAMP authorization and the architectural requirement to transmit metadata to Braintrust's cloud create fundamental blockers for federal adoption</strong>.</p>

            <p>For federal environments, specific due diligence items that must be resolved:</p>

            <p><strong>FedRAMP roadmap and commitment</strong> - Braintrust has no publicly stated FedRAMP authorization timeline. Federal buyers must engage vendor executives to determine if federal market is a strategic priority and what investment Braintrust would make toward compliance. Without vendor commitment, organizations should not invest in extended evaluation.</p>

            <p><strong>Metadata CUI classification</strong> - Organizations must conduct legal review to determine whether project names, experiment identifiers, dataset names, and "metrics and status telemetry" constitute CUI in their operational context. If metadata is CUI, the current hybrid architecture is non-compliant.</p>

            <p><strong>Production air-gapped capability</strong> - Braintrust's "full deployment mode" is explicitly "only available for testing with the intent of using the hybrid configuration in production." This is not a production-ready air-gapped deployment. Organizations need detailed architecture documentation under NDA and vendor commitment to production support for fully self-hosted control plane.</p>

            <p><strong>Data transit documentation gaps</strong> - Braintrust states "metrics and status telemetry" flow from data plane to control plane but does not comprehensively document what data elements this includes. Federal buyers need complete data flow diagrams showing every field transmitted to cloud infrastructure.</p>

            <p>This represents significant evaluation effort with no assurance of favorable outcomes. If Braintrust shows no interest in federal market, organizations should not invest in extended due diligence.</p>

            <div class="callout success">
                <strong>Proceed with pilots only if:</strong>
                <p>Organizations may proceed with limited pilots under these specific conditions: <strong>unclassified data only</strong> where cloud processing of metadata is explicitly permissible under data handling policies; <strong>commercial-side workloads</strong> with no CUI, PHI, or federal data protection requirements; <strong>organization accepts potential future migration</strong> if federal compliance requirements cannot be satisfied and tool must be replaced; <strong>vendor demonstrates concrete interest</strong> in federal market through executive engagement, even if FedRAMP timeline is distant; <strong>pilots remain scoped to evaluation/observability</strong> rather than becoming critical production dependencies until compliance path is clear.</p>
                <p>Federal organizations should treat Braintrust as experimental technology for unclassified innovation projects while investing in relationships with vendors committed to federal compliance or open-source alternatives that can be fully self-hosted without cloud dependencies.</p>
            </div>
        </section>

        <hr>

        <section>
            <h2>Technical Architecture: Marketing Claims vs. Documented Reality</h2>

            <p>Braintrust positions itself with AI-forward messaging emphasizing "Loop" (an AI agent for automated evaluation) and "Brainstore" (a specialized database for AI logs). However, technical documentation reveals the platform is <strong>primarily an infrastructure and workflow orchestration layer</strong> rather than a source of proprietary AI innovation.</p>

            <p>The underlying technology stack consists of standard enterprise components: <strong>Brainstore is a Rust binary that uses existing PostgreSQL and Redis databases plus object storage</strong> (S3-compatible). This represents database optimization work, not AI/ML innovation. The <strong>AI Proxy is an open-source wrapper providing unified API access to third-party LLM providers</strong> (OpenAI, Anthropic, Google, AWS, Mistral, Together) - this is API aggregation, not proprietary model development. Deployment infrastructure uses Lambda functions (AWS), PostgreSQL, Redis, VPC networking, and Terraform/Docker orchestration. Client SDKs support TypeScript and Python for logging and evaluation instrumentation.</p>

            <p>Braintrust's primary AI-branded feature is <strong>"Loop,"</strong> described as "an AI agent that automates the most time-intensive parts of AI development" including generating evaluation datasets, optimizing prompts by analyzing context, and building scorers with custom rubrics. <strong>Loop is currently in public beta and operates as an AI-powered assistant feature</strong> rather than a proprietary model. The feature appears to use third-party LLMs (likely GPT-4 or similar) to analyze user data and generate suggestions. No technical documentation substantiates proprietary ML models, training data, novel architectures, peer-reviewed research, or independent benchmarks demonstrating Loop's AI capabilities beyond standard LLM application patterns.</p>

            <p>For scoring and evaluation, documentation explicitly states users <strong>"Choose the right model for judging"</strong> for LLM-as-a-judge evaluations, confirming Braintrust does <strong>not provide proprietary scoring models</strong>. Users bring their own LLM providers (OpenAI, Anthropic, etc.) for AI-based evaluation. The platform remains <strong>model-agnostic</strong> - a wrapper/orchestrator rather than an AI provider.</p>

            <p><strong>Where Braintrust delivers verified value</strong>: The platform provides <strong>workflow automation through its systematic evaluation framework</strong> (Data + Task + Scorers) that transforms ad-hoc testing into reproducible experiments. <strong>Infrastructure optimization via Brainstore</strong> claims ~80x faster queries for AI trace data versus general-purpose databases (vendor-reported, no independent benchmarking). The <strong>hybrid deployment architecture</strong> separates data plane (customer-hosted) from control plane (Braintrust cloud) for data residency. <strong>Collaborative tooling</strong> enables no-code playground for non-technical users to test prompts alongside engineering workflows. <strong>CI/CD integration</strong> via <code>braintrust eval</code> command integrates evaluations into development pipelines. <strong>OpenTelemetry compatibility</strong> provides standards-based instrumentation for multi-tool ecosystems.</p>

            <p><strong>Unverified claims</strong> include the "80x faster" query performance metric without independent benchmarking or methodology disclosure, and "30%+ accuracy improvements" that likely reflect systematic evaluation versus no evaluation rather than Braintrust-specific AI capabilities. Loop AI capabilities lack technical documentation of underlying models, training approaches, or performance benchmarks distinguishing Loop from general-purpose LLM assistants.</p>

            <p>The competitive moat comes from <strong>enterprise customer relationships</strong> (Notion, Stripe, Zapier creating network effects), <strong>venture backing</strong> ($45M funding from a16z, Greylock), <strong>integrated toolchain</strong> (combined evaluation + observability + playground reducing tool sprawl), and <strong>developer experience</strong> (SDK quality and documentation creating switching costs). This defensibility stems from <strong>packaging, integration quality, enterprise relationships, and infrastructure optimization</strong> rather than proprietary ML intellectual property.</p>

            <div class="callout critical">
                <strong>Federal buyers should understand:</strong>
                <p>Organizations evaluating Braintrust are paying for <strong>workflow automation infrastructure and platform integration</strong>, not breakthrough AI capabilities. Braintrust proxies existing LLMs (you still pay OpenAI/Anthropic), evaluation logic uses code-based scorers you write or LLM-as-a-judge where you provide the model, and Loop automation uses third-party LLMs to assist with dataset/prompt generation. Value assessment should focus on time savings from systematic evaluation versus ad-hoc testing, infrastructure cost reduction from optimized database, team collaboration improvements, and CI/CD integration reducing manual testing overhead. Federal buyers should <strong>not</strong> assess Braintrust based on proprietary AI differentiation, unique model capabilities unavailable elsewhere, or competitive AI performance advantages - the platform uses the same third-party LLMs as alternatives.</p>
                <p>The platform delivers genuine infrastructure value through workflow automation and tooling quality, but federal buyers should evaluate it as <strong>DevOps infrastructure for AI evaluation</strong> rather than AI innovation.</p>
            </div>
        </section>

        <hr>

        <section>
            <h2>Local Development: What Works and What Doesn't</h2>

            <p>This section addresses the core question for FedRAMP compliance: Can Braintrust operate entirely on-premise, potentially avoiding FedRAMP authorization requirements?</p>

            <p>Braintrust provides a <strong>hybrid deployment model</strong> with self-hosted data plane capabilities. For AWS deployment, this includes Lambda functions, PostgreSQL database, and VPC networking via Terraform module. For other environments, Docker deployment provides containerized deployment (<code>standalone-api</code>, <code>postgres</code>, <code>redis</code> containers). <strong>Application data (experiment inputs/outputs, logs, traces, dataset records, prompt playground completions, human review scores) remains in customer infrastructure</strong>. The UI requests use CORS to query the data plane directly, bypassing Braintrust servers. TypeScript/Python SDKs send events directly to the customer-hosted data plane. Customer infrastructure can be protected by firewall/VPN, Braintrust employees do not require access to the data plane, and organizations have database-level control for GDPR/data purging compliance.</p>

            <p><strong>However, core platform functionality requires cloud connectivity to Braintrust's control plane.</strong> The Braintrust web application (braintrustdata.com/app) is hosted by Braintrust - while UI requests query the data plane directly, the UI itself loads from Braintrust's cloud. The following data elements reside in Braintrust's cloud control plane: <strong>experiment and dataset names</strong> (not content, but identifiers), <strong>project names and settings</strong>, <strong>organization information and user accounts</strong>, <strong>API keys</strong> (hashed, but stored in Braintrust cloud), and <strong>encrypted LLM provider secrets</strong>. The data plane transmits <strong>"metrics and status telemetry"</strong> back to the control plane, though documentation does not comprehensively define what constitutes "metrics and status telemetry."</p>

            <p>Loop AI agent functionality (automated dataset generation, prompt optimization) likely requires connectivity to Braintrust services and third-party LLM APIs, though documentation does not specify whether Loop can operate fully offline. New UI features, Loop improvements, and platform capabilities require the cloud-hosted control plane. <strong>Developers cannot access the Braintrust web UI or use collaborative playground features without internet connectivity to Braintrust's cloud control plane.</strong> While local code and application data remain on-premise, the management interface, experiment orchestration, and Loop AI capabilities flow through Braintrust's infrastructure.</p>

            <p><strong>AI inference location</strong>: Braintrust does <strong>not perform AI inference</strong>. The platform proxies requests to third-party LLM providers (OpenAI, Anthropic, etc.). Inference occurs wherever those providers operate (typically their cloud infrastructure), not locally unless users deploy self-hosted LLM endpoints.</p>

            <p><strong>Air-gapped capability</strong>: The hybrid deployment model is <strong>not compatible with air-gapped environments</strong> under the documented production architecture. Braintrust offers a "full deployment mode" (control plane + data plane self-hosted) but <strong>explicitly frames this as "only available for testing with the intent of using the hybrid configuration in production."</strong> This is a testing environment to validate functionality before deploying the production hybrid architecture - not a production-ready air-gapped deployment option. Organizations requiring air-gapped production deployment need vendor engagement under NDA to understand if this capability will ever be production-supported.</p>
        </section>

        <hr>

        <section>
            <h2>Enterprise Fit: Commercial vs. Federal</h2>

            <p>Braintrust excels for <strong>cloud-first organizations with modern DevOps practices</strong> already using AWS/GCP/Azure with Terraform/Kubernetes infrastructure and CI/CD pipelines (Jenkins, GitHub Actions, GitLab CI) seeking evaluation integration. The platform works well for companies comfortable with hybrid SaaS models (data local, control plane cloud). Native OpenTelemetry support, AI SDK integration (Vercel AI SDK, LangChain), direct provider integrations (OpenAI, Anthropic, Google, Mistral), and CI/CD tooling create a strong integration ecosystem.</p>

            <p>The target customer profile includes <strong>product-focused companies embedding AI into core workflows</strong> and needing accuracy monitoring. Case studies from Notion (multi-capability AI platform requiring systematic evaluation across use cases), Stripe, Zapier, Instacart, and Airtable demonstrate enterprise organizations with dedicated AI engineering teams transitioning from ad-hoc testing to systematic evaluation. SOC 2 Type II compliance, hybrid deployment model supporting Fortune 500 data residency needs, collaborative tooling supporting cross-functional teams, and vendor reports of customers managing "tens of thousands of test cases" validate operational maturity and production-scale capability.</p>

            <p><strong>For federal environments, Braintrust faces architectural constraints requiring fundamental product changes:</strong></p>

            <p><strong>Air-gapped networks</strong> represent an insurmountable obstacle under current documented architecture. Hybrid deployment requires connectivity between data plane and control plane, the web UI loads from Braintrust cloud, and the "full deployment mode" is explicitly for testing only, not production air-gapped capability.</p>

            <p><strong>Zero-cloud-data-transit policies</strong> create an insurmountable obstacle. Metadata (project names, experiment names, organization info) flows to Braintrust control plane, and "metrics and status telemetry" transmits from data plane to control plane with undefined scope. Even with application data local, metadata transit violates zero-cloud-data-transit policies unless full self-hosted deployment becomes production-supported.</p>

            <p><strong>Classified workloads</strong> face insurmountable obstacles from lack of FedRAMP authorization, cloud metadata transmission incompatible with classified data handling, and no evidence of vendor interest in classified environment support.</p>

            <p><strong>Intellectual property exposure</strong> presents a configuration-dependent concern. Project names, dataset names, experiment names may reveal IP (e.g., "fraud-detection-v3" or "customer-churn-prediction"). Application code, prompts, and outputs remain in customer data plane, but organizations must assess whether metadata naming conventions expose sensitive IP. Careful naming conventions can mitigate this, but metadata transmission remains.</p>

            <p><strong>PHI/compliance isolation</strong> creates an insurmountable obstacle. Healthcare organizations under HIPAA cannot transmit patient-related metadata to non-compliant cloud, and even de-identified metadata may violate BAA requirements.</p>

            <p><strong>Zero-trust architectures</strong> present configuration-dependent concerns. Outbound connectivity from customer data plane to Braintrust control plane may conflict with zero-trust egress policies. Network architecture requires trust boundary between customer VPC and Braintrust cloud. This is solvable for organizations allowing selective cloud connectivity but represents an architectural constraint for strict zero-trust implementations.</p>

            <p>Federal challenges are primarily <strong>architectural constraints requiring fundamental product changes</strong> rather than configuration issues. Braintrust's hybrid model is designed for commercial enterprises with cloud connectivity, not federal zero-trust or air-gapped environments.</p>
        </section>

        <hr>

        <section>
            <h2>Onboarding and Implementation</h2>

            <p>Organizations can validate basic functionality quickly through multiple entry points. The <strong>web playground enables prompt testing and model comparison within minutes</strong> with no infrastructure setup. <strong>SDK integration</strong> (Python/TypeScript) requires simple <code>Eval()</code> function with three parameters (data, task, scorers) providing automatic experiment tracking and UI integration within hours for first programmatic evaluation. <strong>CI/CD integration</strong> via <code>braintrust eval</code> CLI command in pipelines with custom scorers and OpenTelemetry instrumentation takes days for production integration. A <strong>1-2 day POC timeline</strong> is realistic: day one covers account creation (free tier), UI exploration, simple prompt testing; day two adds SDK integration, first code-based evaluation, and experiment comparison.</p>

            <p>For production integration with hybrid deployment, organizations need moderate DevOps investment. Infrastructure setup includes Terraform deployment (AWS) or Docker orchestration (other clouds) taking 1-2 days, plus PostgreSQL + Redis + object storage provisioning and VPC networking/security group configuration. CI/CD integration involves SDK installation in application codebases, evaluation scripts in test suites, <code>braintrust eval</code> in CI/CD pipelines, and authentication configuration (API keys, service tokens). Organizations pursuing fully self-hosted deployment face operational uncertainty around self-hosted control plane, though Braintrust explicitly positions this as testing-only rather than production deployment.</p>

            <p>Organizations should anticipate <strong>phased deployment</strong>: POC validation (1-2 weeks) using free tier with 2-3 pilot use cases and team familiarization; production integration (4-8 weeks) including hybrid data plane deployment if required, CI/CD pipeline integration, team training, and initial dataset creation from production logs; enterprise rollout (3-6 months) with expanded use case coverage, cross-functional adoption, Loop AI agent exploration (currently beta), and advanced features. This phased approach enables incremental investment rather than requiring complete infrastructure buildout upfront.</p>
        </section>

        <hr>


        <section>
            <h2>Value Proposition and ROI</h2>

            <p>Vendor-reported performance metrics include <strong>~30%+ improvements in AI application accuracy</strong> within weeks of adoption, with Notion's case study showing accuracy improvements "from below 40% to over 80%" using systematic evaluation. These gains reflect transition from ad-hoc testing to comprehensive evaluation frameworks rather than Braintrust-specific AI capabilities. <strong>Brainstore database claims ~80x faster queries</strong> for AI trace data compared to general-purpose databases (vendor-reported without independent benchmarking or methodology disclosure). Vendor cites customers reducing <strong>evaluation time by ~50%</strong> with Loop-assisted scorer editing and eval playground workflows compared to code-only evaluation development.</p>

            <p>These results reflect optimal deployment conditions in organizations with mature DevOps practices and AI expertise. Actual outcomes vary significantly by application complexity (simple classification vs. multi-step agent workflows), existing evaluation maturity (no testing vs. some ad-hoc testing), team expertise (AI engineering sophistication), and data quality (production trace volume and diversity).</p>

            <p><strong>Conservative ROI calculation for a typical AI engineer costing ~$800/day (loaded)</strong>: Systematic evaluation versus ad-hoc testing saves ~20 days annually in reduced manual testing, trace analysis, and debugging (~$16,000 per engineer). Infrastructure time savings of ~5 days annually avoiding custom evaluation infrastructure development (~$4,000 per engineer). Preventing 2-3 significant AI accuracy regressions per year (~$10,000 each in customer impact and remediation) adds ~$20,000-30,000 annually. Vendor claims ~50% reduction in evaluation development time enabling more rapid experimentation translates to 10-15 additional experiments annually with variable but potentially significant competitive advantages. <strong>Total annual benefit per AI engineer: conservative estimate ~$45,000-60,000</strong> under optimal conditions, though actual results vary significantly by organization.</p>

            <p>Beyond cost reduction, Braintrust enables <strong>strategic capabilities</strong>: reproducible evaluation through version-controlled datasets and experiments supporting audit trails, cross-functional collaboration where product managers and domain experts contribute to evaluation without code, continuous quality monitoring via production trace analysis identifying regressions before customer impact, and systematic improvement through data-driven prompt optimization replacing subjective assessment.</p>

            <p><strong>Total cost of ownership</strong> includes licensing ($249/month Pro tier for 5 GB data and 50K scores, or custom Enterprise pricing for hybrid deployment), infrastructure costs for hybrid deployment (PostgreSQL + Redis + object storage + Lambda/compute ~$500-2,000/month depending on scale), migration effort (2-4 weeks engineering time for CI/CD integration and dataset creation, ~$16,000-32,000 one-time), minimal training (1-2 days team onboarding, ~$5,000), and continued LLM provider costs (Braintrust is model-agnostic - organizations continue paying OpenAI, Anthropic, etc.). For teams with 3+ AI engineers, conservative ROI estimates suggest <strong>2-4 month payback</strong> through reduced testing overhead and infrastructure time savings.</p>
        </section>

        <hr>

        <section>
            <h2>Key Benefits and Critical Blockers</h2>

            <p><strong>Strategic value</strong> comes from vendor-reported ~30%+ accuracy improvements in AI applications within weeks (Notion case study: 40% → 80% accuracy), ~80x faster queries for AI trace data via Brainstore database optimization, ~50% evaluation development time reduction with Loop AI assistant and eval playground, enterprise credibility through deployment by Notion, Stripe, Zapier, Airtable, Instacart, Vercel, Coda, venture backing with $150M valuation and $45M funding from Andreessen Horowitz (a16z), Greylock, Datadog, Databricks, and operational maturity demonstrated by customer base doubling in past year with customers managing "tens of thousands of test cases." All performance metrics represent vendor-reported results under optimal conditions.</p>

            <p><strong>Operational advantages</strong> include no-code accessibility via playground enabling product managers and domain experts to test prompts without engineering involvement, systematic evaluation framework ("Data + Task + Scorers" methodology) creating reproducible version-controlled testing, CI/CD automation via <code>braintrust eval</code> command integrating evaluations into development pipelines, hybrid deployment flexibility for data residency (though metadata in cloud), model-agnostic operation across OpenAI, Anthropic, Google, Mistral, local models via unified proxy, OpenTelemetry compatibility for standards-based instrumentation in multi-tool ecosystems, collaborative workflows with shared experiments enabling cross-functional debugging and analysis, and Loop AI automation (beta) for automated dataset generation, prompt optimization, and scorer creation reducing manual evaluation overhead.</p>

            <p><strong>Technical limitations</strong> include <strong>cloud-dependent architecture</strong> where core platform functionality (UI, Loop, experiment orchestration) requires connectivity to Braintrust control plane in cloud. <strong>Metadata transmission to cloud</strong> means project names, experiment names, dataset names, organization info, "metrics and status telemetry" flow to Braintrust infrastructure even with hybrid deployment. <strong>No true air-gap support</strong> exists for production deployments - hybrid deployment requires data plane ↔ control plane connectivity, and "full deployment mode" is explicitly for testing only. <strong>Third-party LLM dependency</strong> means AI inference occurs at LLM provider infrastructure (OpenAI, Anthropic clouds) unless organizations deploy self-hosted models. <strong>Brainstore database dependency</strong> requires PostgreSQL + Redis + S3-compatible object storage for self-hosted data plane.</p>

            <p><strong>Federal and enterprise concerns</strong> center on <strong>no FedRAMP authorization</strong> (Braintrust is not Ready, In Process, or Authorized on FedRAMP marketplace with no public commitment to federal compliance). <strong>Classified workload incompatibility</strong> stems from cloud metadata transmission and lack of FedRAMP making classified environments impossible under current architecture. <strong>CUI metadata exposure</strong> means for federal workloads, project/experiment/dataset names likely constitute Controlled Unclassified Information requiring protection. <strong>Intellectual property exposure</strong> occurs when metadata naming conventions may reveal sensitive business logic (e.g., "fraud-detection-model-v3"). <strong>PHI/HIPAA concerns</strong> prevent healthcare organizations from transmitting patient-related metadata to non-BAA infrastructure. Vendor lock-in exists where proprietary APIs for Loop, playground, Brainstore create migration friction, though OpenTelemetry and PostgreSQL provide some portability.</p>

            <p><strong>Roadmap and federal alignment</strong> show commercial market focus through customer base (Notion, Stripe, Zapier) indicating prioritization of cloud-native enterprises, not federal agencies. <strong>No public FedRAMP roadmap</strong> exists - vendor has not announced federal compliance intentions or timeline. <strong>Insufficient air-gap documentation</strong> with "full deployment mode" mentioned but explicitly positioned as testing-only suggests federal scenarios are non-priority. <strong>Metadata cloud dependency by design</strong> indicates hybrid architecture appears optimized for commercial data residency (GDPR) rather than federal zero-cloud-transit requirements.</p>

            <p>Determining federal viability requires significant technical due diligence with no assurance of favorable outcomes: vendor engagement to assess federal market interest and FedRAMP investment willingness, architecture review under NDA to validate fully self-hosted control plane capability for production (not just testing), legal analysis of metadata CUI classification in specific federal contexts, and security review of data flows between data plane and control plane. Organizations requiring federal compliance should <strong>not invest in extended evaluation until vendor demonstrates concrete interest</strong> in addressing blockers. Without FedRAMP commitment, Braintrust remains incompatible with federal requirements.</p>
        </section>

        <hr>

        <section>
            <h2>Competitive Analysis</h2>

            <p><strong>Versus traditional manual testing</strong>, Braintrust provides systematic evaluation with reproducible experiments versus ad-hoc prompt testing, collaboration enabling cross-functional teams testing via UI versus engineering-only workflows, scale through automated evaluation across thousands of test cases versus manual spot-checking, traceability via version-controlled datasets and experiments versus undocumented testing, and speed with ~80x faster queries (vendor claim) and ~50% evaluation development time reduction. However, <strong>manual testing works air-gapped while Braintrust requires connectivity</strong>, <strong>manual testing keeps all data local while Braintrust transmits metadata to cloud</strong>, <strong>manual testing has zero infrastructure while Braintrust requires deployment</strong>, and <strong>manual testing is free while Braintrust charges $249+/month</strong> (Pro tier). The tradeoff: productivity gains (systematic workflow, faster iteration) versus deployment constraints (cloud dependency, metadata transmission). <strong>For federal environments with zero-cloud-transit policies, manual testing may be superior despite lower efficiency because it satisfies fundamental compliance requirements Braintrust cannot meet.</strong></p>

            <p><strong>Versus AI-native competitors</strong>, Braintrust versus LangSmith shows similar hybrid models (data local, control plane cloud) with neither having FedRAMP authorization as of January 2025. Braintrust offers stronger TypeScript/JavaScript support and collaborative playground while LangSmith tightly integrates with LangChain ecosystem. Braintrust versus Arize Phoenix reveals Phoenix is <strong>fully open-source allowing complete self-hosting</strong> while Braintrust is closed-source SaaS. Phoenix has 50+ instrumentations and stronger agent tracing while Braintrust has 5 instrumentations (per competitive analysis). <strong>Phoenix's open-source model enables true air-gapped deployment while Braintrust's cloud dependency is an architectural limitation</strong>. Phoenix requires self-operation or Arize AX managed service while Braintrust offers managed SaaS. Braintrust's competitive positioning emphasizes rapid prototyping where playground and Loop (beta) excel for experimentation versus Phoenix's observability focus, enterprise polish through SOC 2 Type II, hybrid deployment, and collaborative tooling versus open-source operational overhead, and developer experience via TypeScript/Python SDKs and documentation quality creating good onboarding.</p>

            <p><strong>The fundamental tradeoff</strong>: Cloud-dependent architecture enables sophisticated collaborative capabilities but restricts deployment to environments permitting metadata transmission to vendor infrastructure. Braintrust's hybrid model addresses commercial data residency (GDPR, enterprise policies) but does not satisfy federal zero-cloud-transit or air-gapped requirements. The control plane cloud dependency creates both advantages (managed UI, Loop automation, rapid feature development) and limitations (metadata exposure, FedRAMP blocker, air-gap incompatibility).</p>

            <p><strong>Key insight for federal environments</strong>: Traditional approaches or open-source alternatives may prove superior for federal use cases despite lower productivity, because they satisfy fundamental deployment constraints that Braintrust cannot meet. The "better" tool becomes unusable if it violates security policies. For classified or strict zero-cloud environments, <strong>Arize Phoenix (open-source) or manual evaluation frameworks may be the only viable options</strong> until Braintrust achieves FedRAMP authorization and true air-gap capability for production deployments.</p>
        </section>

        <hr>

        <section>
            <h2>Recommended Actions</h2>

            <p><strong>Initial discovery</strong> for federal organizations represents significant effort with uncertain outcomes. Technical deep-dive requirements include vendor engagement to document complete data flows (every metadata field transmitted to cloud), architecture review under NDA to validate fully self-hosted control plane capability for production (not testing), testing of "full deployment mode" to understand limitations and feature gaps, and network traffic analysis to identify all connectivity requirements. This requires 20-40 hours technical work plus legal review (~$15,000-30,000 internal cost). Vendor engagement topics include federal market priority and FedRAMP roadmap/timeline, investment willingness for compliance (FedRAMP, IL-specific features), production deployment documentation and support commitment for fully self-hosted architecture, and metadata transmission scope and optionality. This demands executive-level meetings requiring senior stakeholder time. Security and compliance reviews need legal analysis of metadata CUI classification, data flow mapping for compliance documentation, risk assessment of cloud metadata transmission, and alternative evaluation (Arize Phoenix, manual frameworks) taking 15-30 hours compliance work (~$10,000-20,000). <strong>Total discovery effort: 40-80 hours (~$30,000-60,000) with no assurance of favorable outcomes.</strong> If vendor shows no federal interest, halt discovery immediately.</p>

            <p>For commercial organizations, discovery is lightweight: free tier account creation and playground exploration (1-2 hours), POC with 2-3 use cases (1-2 days), and hybrid deployment testing if data residency required (3-5 days).</p>

            <p><strong>Evaluation phase</strong> includes POC scope and validation criteria measuring evaluation development time reduction versus current approach, cross-functional collaboration improvement (product/engineering), and query performance for trace analysis. Scope should cover 2-3 representative AI workflows (e.g., RAG Q&A, classification, agent workflow) over 4-6 weeks with a team of 2-3 AI engineers plus 1 product stakeholder. Federal viability assessment shows IL-0/Unclassified may be viable if metadata is not CUI and cloud connectivity permitted, IL-2/CUI likely non-viable due to metadata cloud transmission, and IL-4+/Classified non-viable under current architecture. ROI calculation methodology should measure current evaluation time investment (hours per week), pilot Braintrust and measure time savings, calculate annual time savings × engineer cost, add quality improvement value (production issues prevented), and compare to total cost (licensing + infrastructure + migration).</p>

            <p><strong>Strategic development</strong> for organizations pursuing long-term relationship should prioritize feature advocacy for FedRAMP authorization (critical enabler for federal market), fully self-hosted control plane with documented production-ready air-gap deployment eliminating cloud dependency, metadata optionality through configuration to prevent metadata transmission to cloud, and IL-specific deployment guides with federal compliance documentation. Monitor FedRAMP marketplace quarterly for Braintrust authorization status and track competitor certifications (LangSmith, Arize) for market movement. Build internal expertise in AI evaluation methodologies (valuable regardless of tool), develop relationships with Braintrust product team if vendor shows federal interest, and assess open-source alternatives (Arize Phoenix) for federal compliance fallback. Develop skills in systematic evaluation frameworks (Data + Task + Scorers), competency in assessing cloud-dependent tools against federal security requirements, and evaluation criteria for AI observability platforms balancing capability versus compliance.</p>
        </section>

        <hr>

        <section>
            <h2>Decision Framework</h2>

            <div class="callout success">
                <strong>Proceed if:</strong>
                <p>Commercial organization with cloud connectivity and no federal compliance requirements where data residency is satisfied by hybrid deployment (application data local, metadata cloud acceptable), DevOps maturity exists to deploy and operate Terraform/Docker infrastructure, AI development scale justifies evaluation platform investment (3+ AI engineers, multiple AI products), and vendor engagement demonstrates responsiveness and product roadmap alignment.</p>
            </div>

            <div class="callout critical">
                <strong>Halt if:</strong>
                <p>Federal environment requiring FedRAMP authorization or zero-cloud-data-transit policies, air-gapped deployment required with no internet connectivity permitted, classified workloads or strict CUI handling with metadata cloud transmission prohibited, vendor shows no federal interest after initial engagement (don't invest in due diligence), security review reveals unacceptable risks in metadata transmission or data flows, or open-source alternative (Arize Phoenix) meets all requirements with lower risk/cost.</p>
            </div>

            <div class="callout">
                <strong>Wait and Watch if:</strong>
                <p>Federal compliance path unclear but vendor demonstrates interest in exploring requirements, pilot use cases exist for unclassified data where cloud metadata acceptable, competitive landscape evolving with potential for vendor FedRAMP pursuit, or internal evaluation maturity low where organization may not be ready to leverage advanced platform capabilities.</p>
            </div>
        </section>

        <hr>

        <section>
            <h2>Conclusion</h2>

            <p>Braintrust delivers genuine value as an AI evaluation and observability platform, transforming ad-hoc prompt testing into systematic, reproducible workflows with measurable quality improvements. The platform excels for cloud-native organizations with mature DevOps practices, offering collaborative tooling, CI/CD integration, and workflow automation that vendor reports translate to ~30%+ accuracy improvements and significant time savings. Enterprise customers like Notion, Stripe, and Zapier validate the platform's operational maturity and commercial value proposition.</p>

            <p>However, <strong>the lack of FedRAMP authorization and architectural requirement to transmit metadata to Braintrust's cloud create fundamental blockers for federal adoption</strong>. While the hybrid deployment model keeps application data (prompts, outputs, traces) in customer infrastructure, operational metadata (project names, experiment identifiers, dataset names, organization information) flows to non-FedRAMP cloud infrastructure. For federal workloads handling CUI or classified information, this metadata transmission represents a compliance violation that cannot be resolved through configuration. Braintrust's "full deployment mode" for self-hosting both control plane and data plane is <strong>explicitly positioned as "only available for testing with the intent of using the hybrid configuration in production"</strong> - not a production air-gapped deployment option.</p>

            <p><strong>Recommendation</strong>: <strong>Wait and Watch</strong> for federal environments. Do not invest in extended evaluation or deployment until Braintrust demonstrates concrete commitment to FedRAMP authorization and provides production-ready documentation for fully air-gapped self-hosted deployment. Current architecture appears optimized for commercial enterprises with cloud connectivity, not federal zero-trust or classified environments. Organizations requiring federal compliance should evaluate <strong>Arize Phoenix (open-source)</strong> for true air-gapped capability or invest in manual evaluation frameworks that keep all data on-premise.</p>

            <p><strong>Conditional Proceed</strong> for commercial environments where cloud metadata transmission is permissible and systematic AI evaluation delivers strategic value. Focus adoption on unclassified workloads with DevOps maturity to leverage hybrid deployment model.</p>

            <p><strong>Advocacy strategy</strong> (if tool has long-term federal potential): Engage Braintrust executives to assess federal market interest without committing to adoption. Articulate clear federal requirements (FedRAMP, production air-gap support, metadata control) and gauge vendor willingness to invest. If Braintrust shows genuine interest, advocate for FedRAMP authorization as strategic priority (12-18 month timeline), production-ready fully self-hosted control plane eliminating cloud dependency, and phased approach enabling current commercial pilots while monitoring federal progress. This creates optionality for future federal expansion without blocking current commercial value realization. However, if vendor focus remains purely commercial, redirect federal budgets toward compliant alternatives.</p>

            <p><strong>Next Steps</strong>: Commercial organizations should initiate free tier POC to validate evaluation workflow improvements and team collaboration benefits. Federal organizations should engage vendor at executive level to assess federal market priority before investing in technical evaluation. All organizations should develop internal AI evaluation competency (systematic testing, scoring methodologies) valuable regardless of tooling choice. Service providers should build expertise in AI observability platforms and federal compliance requirements to support client evaluation and deployment.</p>
        </section>

        <hr>

        <section>
            <h2>Sources and Methodology</h2>

            <p>This evaluation is based on publicly available information from Braintrust's website, technical documentation, blog posts, case studies, and third-party analysis accessed in January 2025. Performance metrics and deployment claims reflect vendor-reported results under optimal conditions and may not be representative of all customer experiences.</p>

            <p><strong>Key Sources</strong>: Braintrust website (braintrust.dev) for product documentation, architecture guides, and pricing; Braintrust blog for case studies (Notion), hybrid deployment technical details, and feature announcements; Braintrust GitHub repositories for open-source AI proxy, Terraform modules, and SDK documentation; third-party analysis including Arize Phoenix competitive comparison, VentureBeat coverage, and PitchBook funding data; FedRAMP Marketplace (marketplace.fedramp.gov) for authorization status verification (no listing found); industry publications for AI observability platform comparisons and SDLC evolution analysis.</p>

            <p><strong>Research Limitations</strong>: Performance claims based on vendor marketing materials and selected case studies (Notion) with independent benchmarking not available. FedRAMP status assessed from public FedRAMP marketplace only; private compliance discussions may exist but are not documented. Technical architecture details derived from public documentation; internal implementation may differ. Fully self-hosted deployment capability mentioned but documentation explicitly frames "full deployment mode" as testing-only rather than production air-gapped deployment. Metadata transmission scope ("metrics and status telemetry") not comprehensively defined in public documentation. ROI calculations use hypothetical scenarios with conservative estimates; actual results vary by organization, AI maturity, and use case complexity.</p>

            <p><strong>Validation Recommendations</strong>: Before federal adoption decisions, organizations should conduct independent POC testing to validate performance claims (query speed, evaluation time reduction) in representative environment, obtain detailed architecture documentation under NDA to understand complete data flows, metadata transmission, and production-ready fully self-hosted capability, secure direct FedRAMP roadmap commitment from Braintrust executive leadership with timeline and investment details, engage reference customers in regulated industries (healthcare, finance) to understand compliance approaches and limitations, conduct legal review of metadata elements to determine CUI classification in specific federal operational context, and perform network traffic analysis during pilot to document all connectivity requirements and data transmitted to cloud.</p>

            <p>Organizations should not make federal procurement decisions based solely on this public information analysis without conducting thorough vendor engagement and compliance validation.</p>
        </section>

        <footer>
            <p><em>Document Classification: Internal Use Only</em></p>
            <p><em>Research Conducted: January 2025</em></p>
            <p><em>Contact: christopher.g.roge@afs.com</em></p>
            <div>
                <span class="gt-symbol">&gt;</span>
            </div>
        </footer>
    </div>
</body>
</html>
